#################################################
## Part I: estimation and sanity-check of a two-segment logit model
#################################################
#################################################
## Load data
#################################################
library('data.table')
library('mlogit')
library('RColorBrewer')
rcol <- brewer.pal(9, 'Blues')
raw.data <- fread("../data/bubbles.csv", stringsAsFactors=FALSE)
demo <- fread("../data/demo.csv", stringsAsFactors=FALSE)
colnames(raw.data) =
c("id","week","trip","price.1","price.2","price.3","purchase.1","purchase.2","purchase.3")
 # change column names
raw.data$purchase <- 0
raw.data$purchase[raw.data$purchase.1 == 1] <- 1
raw.data$purchase[raw.data$purchase.2 == 1] <- 2
raw.data$purchase[raw.data$purchase.3 == 1] <- 3
#################################################
## Common logit
#################################################
data <- raw.data
data$price.0 <- 0
mldat <- mlogit.data(data = data[, c(1:6, 10, 11)], id.var = "id", choice = "purchase",
shape = "wide", varying = c(4:6, 8))
ml_simp <- mlogit(purchase ~ price, data = mldat)
#################################################
## clustered logit
#################################################
N <- 359
NC <- 6
demo_cluster <- kmeans(demo[, 2:18], centers = NC, nstart = 100) # Naive approach
demo_cluster$centers # examine how we can interpret them
#
# # kick out male and female occupation; keep age
# demo$hh_occ <- NULL
# demo$fem_occup <- NULL
# demo$male_occup <- NULL
# demo$hh_edu <- NULL
# demo$hh_age <- NULL
# # re-do
# demo_cluster <- kmeans(demo[, 2:13], centers = NC, nstart = 100) # Naive approach
# demo_cluster$centers # examine how we can interpret them
cluster_dat <- data.frame(id = demo$id)
cluster_dat$cluster <- demo_cluster$cluster
data <- merge(data, cluster_dat, by = "id", all.x = T)
data$cluster[is.na(data$cluster)] <- 0
coef.est <- data.frame(segment = 0:NC, intercept.KR = NA, intercept.KB = NA,
intercept.MB = NA, price.coef = NA)
se.est <- data.frame(segment = 0:NC, intercept.KR = NA, intercept.KB = NA, intercept.MB
= NA, price.coef = NA)
pred.sh <- data.frame(segment = 0:NC, no_purchase = NA, KR = NA, KB = NA, MB = NA)
# avg.elas <- data.frame(segment = 0:NC, KR = NA, KB = NA, MB = NA, MB_on_KR = NA,
KR_on_KB = NA, KB_on_MB = NA) # a crude measure
avg.deriv <- data.frame(segment = 0:NC, KR = NA, KB = NA, MB = NA, MB_on_KR = NA,
KR_on_KB = NA, KB_on_MB = NA) # a crude measure
seg.share <- c(N - sum(table(demo_cluster$cluster)), table(demo_cluster$cluster)) / N
avg.price <- colMeans(data.sub[data.sub$price.3 != 99, 4:6])
for (seg in 0:NC) {

 data.sub <- subset(data, cluster == seg)
 mldat.sub <- mlogit.data(data = data.sub[, c(1:6, 10, 11)], choice = "purchase",
shape = "wide", varying = c(4:6, 8))
 ml_est_sub <- mlogit(purchase ~ price, data = mldat.sub, start = c(4, 4, 4, -4))
 coef.est[seg + 1, 2:5] <- t(summary(ml_est_sub)$coefficient)
 se.est[seg + 1, 2:5] <- t(sqrt(abs(diag(solve(ml_est_sub$hessian)))))
 share_temp <- colMeans(fitted(ml_est_sub, outcome = FALSE))
 pred.sh[seg + 1, 2:5] <- t(share_temp)
 # average own-price elasticity
 avg.deriv[seg + 1, 2:4] <- (1 - t(share_temp[2:4])) * t(share_temp[2:4]) *
coef.est[seg + 1, 5]
 # average cross-price elasticities
 avg.deriv[seg + 1, 5] <- - share_temp[2] * share_temp[4] * coef.est[seg + 1, 5] #
MB's price on KR's sales
 avg.deriv[seg + 1, 6] <- - share_temp[3] * share_temp[2] * coef.est[seg + 1, 5] #
KR's price on KB's sales
 avg.deriv[seg + 1, 7] <- - share_temp[4] * share_temp[3] * coef.est[seg + 1, 5] #
KB's price on MB's sales
}
plot(coef.est$intercept.KR - coef.est$intercept.KB, coef.est$price.coef, xlab = "Regular
vs Bubble")
# plot(coef.est$intercept.2 - coef.est$intercept.3, coef.est$price.coef, xlab = "Kiwi vs
Mango")
coef.est
pred.sh
avg.share <- colSums(pred.sh * seg.share)
# elasticity!
colSums(avg.deriv * seg.share)[2:7] / c(avg.share[3:5], avg.share[3:5]) * c(avg.price,
avg.price)
#################################################
## halt: check which demographics predict choices well
#################################################
data_aug <- merge(data, demo, by = "id") # drop the missing demographic observations
mldat_augment <- mlogit.data(data = data_aug[, -(7:9)], id.var = "id", choice =
"purchase", shape = "wide", varying = c(4:6, 8))
# # create interactions
# mldat_augment$price_fam_size = mldat_augment$price * mldat_augment$fam_size
# mldat_augment$price_tv = mldat_augment$price * mldat_augment$tv
ml_test <- mlogit(purchase ~ price + I(price * fam_size) + I(price * tv) + I(price *
fem_age) + I(price * fem_educ) + I(price * fem_work) + I(price * child_code), data =
mldat_augment)
ml_test <- mlogit(purchase ~ price + I(price * fam_size) + I(price * fem_age) +
 I(price * fem_educ) + I(price * fem_work) + I(price * fem_smoke) +
I(price * child_code) + I(price * tv)
 , data = mldat_augment)
demo_sub <- demo[, c(1, 2, 9, 10, 18)]
# re-weight these demographics by their importance in the interaction with price
demo_sub$fam_size <- demo_sub$fam_size * abs(ml_test$coefficients[5])
demo_sub$fem_work <- demo_sub$fem_work * abs(ml_test$coefficients[8])
demo_sub$fem_smoke <- demo_sub$fem_smoke * abs(ml_test$coefficients[9])
demo_sub$tv <- demo_sub$tv * abs(ml_test$coefficients[10])
NC <- 6
demo_cluster <- kmeans(demo_sub[, 2:5], centers = NC, nstart = 100) # cluster based
on the chosen set of demographics
demo_cluster$centers # examine how we can interpret them
seg.share <- c(N - sum(table(demo_cluster$cluster)), table(demo_cluster$cluster)) / N

# merge demo
data$cluster <- NULL
cluster_dat <- data.frame(id = demo$id)
cluster_dat$cluster <- demo_cluster$cluster
data <- merge(data, cluster_dat, by = "id", all.x = T)
data$cluster[is.na(data$cluster)] <- 0
coef.est <- data.frame(segment = 0:NC, intercept.KR = NA, intercept.KB = NA,
intercept.MB = NA, price.coef = NA)
for (seg in 0:NC) {

 data.sub <- subset(data, cluster == seg)
 mldat.sub <- mlogit.data(data = data.sub[, c(1:6, 10, 11)], choice = "purchase",
shape = "wide", varying = c(4:6, 8))
 ml_est_sub <- mlogit(purchase ~ price, data = mldat.sub, start = c(4, 4, 4, -4))
 coef.est[seg + 1, 2:5] <- t(summary(ml_est_sub)$coefficient)
}

# now because we can interpret the segments, compare with segments
res <- cbind(coef.est, seg.share, rbind(0, demo_cluster$centers))
res
plot(coef.est$intercept.KR - coef.est$intercept.KB, coef.est$price.coef, xlab = "Regular
vs Bubble")
# plot(coef.est$intercept.2 - coef.est$intercept.3, coef.est$price.coef, xlab = "Kiwi vs
Mango")
# shares and elasticities
calc_shares <- function(coefs, p) {

 nobs <- dim(p)[1]
 intercept <- coefs[1:3] # in this case, we have product-specific intercepts
 price.coef <- coefs[4]

 utility <- rep(1, nobs) %*% t(intercept) + # brand-specific intercept
 p * price.coef # price coefficients

 shares <- exp(utility) / (1 + rowSums(exp(utility)))

 return(shares)
}
price_mat <- data[, 4:6]
price_mat[price_mat == 99] <- NA
mean_price <- colMeans(price_mat, na.rm = T)
shares_mat <- data.frame(segment = 0:NC, share_KR = NA, share_KB = NA, share_MB = NA)
for (i in 0:NC) {
 shares_mat[i+1, 2:4] <- calc_shares(t(coef.est[i+1, 2:5]), t(mean_price))
}
# show this with coefficients
res <- cbind(coef.est, seg.share, shares_mat[, 2:4])
res
# make this one function
calc_shares_seg <- function(coef_mat, p, seg.share) {

 nobs <- dim(p)[1]

 # calculate share for each segment
 shares_mat <- data.frame(segment = 0:NC, share_KR = NA, share_KB = NA, share_MB =
NA)
 for (i in 0:NC) {
 shares_mat[i+1, 2:4] <- calc_shares(t(coef.est[i+1, 2:5]), p)
 }

 # take weighted avg
 share_mkt <- colSums(as.matrix(shares_mat[, 2:4]) * seg.share)

 return(share_mkt)

}
calc_shares_seg(coef.est, t(mean_price), seg.share) # pretty symmetric shares
p_0 <- mean_price
s_0 <- calc_shares_seg(coef.est, t(p_0), seg.share)
p_1 <- mean_price + c(0.01, 0, 0)
s_1 <- calc_shares_seg(coef.est, t(p_1), seg.share)
(s_1 - s_0) / s_0 / ((p_1[1] - p_0[1]) / p_0[1]) # own and cross price elasticities
for KR
# recover the full elasticity matrix
elas_mat <- matrix(nrow = 3, ncol = 3)
row.names(elas_mat) <- c("KR", "KB", "MB")
p_0 <- mean_price
s_0 <- calc_shares_seg(coef.est, t(p_0), seg.share) # pretty symmetric shares
for (i in 1:3) {

 # price and shares
  p_1 <- p_0
 p_1[i] <- p_1[i] + 0.01 # perturb the ith product
 s_1 <- calc_shares_seg(coef.est, t(p_1), seg.share) # pretty symmetric shares

 # calculate elasticities
 elas_mat[i, ] <- t((s_1 - s_0) / s_0 / ((p_1[i] - p_0[i]) / p_0[i]))

}
elas_mat
# profit
mc <- c(0.5, 0.5, 0.5)
calc_profit <- function(p) {

 # shares
 shares <- calc_shares_seg(coef.est, t(p), seg.share)

 # profit
 profit <- shares * t(p - mc)
 colnames(profit) <- c("KR", "KB", "MB")
 return(profit)

}
calc_profit(mean_price)
# profit maximization, for Kiwi
kiwi_opt <- optim(par = mean_price[1:2], fn = function(p_kiwi) {
 profit <- calc_profit(c(p_kiwi, mean_price[3])) # focus on Kiwi's price, but bind
Mango Bubble's price
 return(-sum(profit[1:2])) # negative of Kiwi's profit
})
kiwi_opt$par
mango_opt <- optim(par = mean_price[3], fn = function(p_mango) {
 profit <- calc_profit(c(mean_price[1:2], p_mango)) # focus on Mango's price, but
bind Mango Bubble's price
 return(-profit[3]) # negative of Mango's profit
})
##########################
# latent segment version
##########################
likelihood <- function(coefs, p, choice, id) {

 # use calc_shares()
 shares <- calc_shares(coefs, p)

 # likelihood of individual-trip
 # last term because utility does not contain no-purchase
 likelihood_trip <- shares[, 1] * (choice == 1) + 
  shares[, 2] * (choice == 2) +
 shares[, 3] * (choice == 3) +
 (1 - rowSums(shares)) * (choice == 0)

 # product of likelihood to get likelihood for an individual, across trips
 likelihood_ind <- aggregate(x = likelihood_trip, by = list(id), FUN = prod)
 return(likelihood_ind)

}
logit.two.type <- function(coef.1, coef.2, typeprob, p, choice, id) {

 # first part are essentially simple logit for each type
 # (ultimately, find individual likelihood across all trips)
 likelihood.1 <- likelihood(coef.1, p, choice, id)
 likelihood.2 <- likelihood(coef.2, p, choice, id)

 # second part averages across types and sum up log likelihood across individuals
 likelihood.ind <- likelihood.1[, 2] * typeprob +
 likelihood.2[, 2] * (1 - typeprob)

 # delete extreme values such as zero (technical detail)
 likelihood.ind[likelihood.ind == 0] <- 1E-1000

 # aggregate across individuals
 neg.log.likelihood <- - sum(log(likelihood.ind))
 return(neg.log.likelihood)

}
# extract price matrix and choice vector
p <- as.matrix(raw.data[, 4:6])
choice <- raw.data$purchase
id <- raw.data$id
# use startval that make sense (and check sensitivity)
startval <- c(3, 3, 3, -2, 5, 5, 5, -4, 0.2)
# estimation by maximum likelihood
estimates <- optim(startval, function(coef)
 logit.two.type(coef.1 = coef[1:4], coef.2 = coef[5:8],
 typeprob = coef[9], p = p, choice = choice, id = id),
 lower = c(rep(-Inf, 8), 0), upper = c(rep(Inf, 8), 1),
 method = "L-BFGS-B")
# note: the upper and lower bound for the type probability is crucial
# because optimizer frequently goes over the bound and this is meaningless
# results, in a nicer way
res <- cbind(c(estimates$par[9], estimates$par[1:4]), c(1 - estimates$par[9],
estimates$par[5:8]))
row.names(res) <- c("seg.share", "KR", "KB", "MB", "price")
res
# assign some parameters for easy interpretation
beta.1 <- estimates$par[1:3]
alpha.1 <- estimates$par[4]
beta.2 <- estimates$par[5:7]
alpha.2 <- estimates$par[8]
type.prob <- estimates$par[9]
# predict shares at the price of $1.43 (close to the average price)
price.list <- t(c(1.43, 1.43, 1.43))
# overall market share: sum over segments
calc_shares_seg <- function(coef.1, coef.2, p, type.prob) {

 # share for both types
 share.1 <- calc_shares(coef.1, p)
 share.2 <- calc_shares(coef.2, p)

 # calculate share for each segment
 share_mkt <- type.prob * share.1 + (1 - type.prob) * share.2
 return(share_mkt)

}
# initialize the elasticity matrix
elas_mat <- matrix(nrow = 3, ncol = 3)
row.names(elas_mat) <- c("KR", "KB", "MB")
p_0 <- price.list
s_0 <- calc_shares_seg(c(beta.1, alpha.1), c(beta.2, alpha.2), p_0, type.prob)
### full profit optimization
# cost for all products
mc <- 0.5
# define profit of all products as a function of all prices
profit.sim <- function(p) {
 share_mkt <- calc_shares_seg(c(beta.1, alpha.1), c(beta.2, alpha.2), p, type.prob)
 profit <- share_mkt * (p - mc)
 return(profit)
}
# optimize Kiwi's profit by solving for the prices of KR and KB
optim.kiwi.profit <- optim(c(1.43, 1.43), function(prices)
 -sum(profit.sim(t(c(prices, 1.43)))[1:2])) # sum over the first two product's
profit, i.e. KR and KB
# prices: KR, then KB
optim.kiwi.profit$par
# profit per 1000 customer-trips
- optim.kiwi.profit$value * 1000
### model without segments?
# take coefficients
coef_simp <- ml_simp$coefficients
# define profit of all products as a function of all prices
profit.sim.simp <- function(p) {
 share_mkt <- calc_shares_seg(coef_simp, coef_simp, p, 0.5) # just stick in the
same coefficient for both types
 profit <- share_mkt * (p - mc)
 return(profit)
}
# optimize Kiwi's profit by solving for the prices of KR and KB
optim.kiwi.simp <- optim(c(1.43, 1.43), function(prices) 
 -sum(profit.sim.simp(t(c(prices, 1.43)))[1:2])) # sum  over the first two
product's profit, i.e. KR and KB
# prices: the same price because on the aggregate, demand seems similar
optim.kiwi.simp$par
# profit without segmentation
profit.1 <- profit.sim(t(c(optim.kiwi.simp$par, 1.43)))
# profit with segmentation
profit.2 <- profit.sim(t(c(optim.kiwi.profit$par, 1.43)))
profit.stacked <- t(rbind(profit.1, profit.2)) * 1000
colnames(profit.stacked) <- c('without seg.', 'with seg.')
barplot(profit.stacked, col = rcol[c(7, 4, 1)], ylab = 'profit of KR, KB, MB')
#################################################
## logit with past purchase
#################################################
# for-loop to calculate last_purchase (see if you have a better way of doing this!!)
data$last_purchase <- 0
data$purchased_last_trip <- 0 # to test inventory
for (i in unique(data$id)) {
 for (t in unique(subset(data, id == i)$trip)[-1]) {
 data[id == i & trip == t, ]$last_purchase = (data[id == i & trip == t-1,
]$last_purchase) * (data[id == i & trip == t-1, ]$purchase == 0) +
 (data[id == i & trip == t-1, ]$purchase) * (data[id == i & trip == t-1,
]$purchase > 0)
 data[id == i & trip == t, ]$purchased_last_week = as.numeric(data[id == i & trip
== t-1, ]$purchase > 0)
 }
}
data$last_purchase.0 <- 0
data$last_purchase.1 <- as.numeric(data$last_purchase == 1)
data$last_purchase.2 <- as.numeric(data$last_purchase == 2)
data$last_purchase.3 <- as.numeric(data$last_purchase == 3)
coef.est.2 <- data.frame(segment = 0:NC, intercept.1 = NA, intercept.2 = NA, intercept.3
= NA, price.coef = NA, last.purchase = NA)
se.est.2 <- data.frame(segment = 0:NC, intercept.1 = NA, intercept.2 = NA, intercept.3 =
NA, price.coef = NA, last.purchase = NA)
for (seg in 0:NC) {

 data.sub <- subset(data, cluster == seg)
 mldat.sub <- mlogit.data(data = data.sub[, c(1:6, 10, 11, 14:18)], choice =
"purchase", shape = "wide", varying = c(4:6, 8, 10:13))
 ml_est_sub <- mlogit(purchase ~ price + last_purchase, data = mldat.sub, start =
c(4, 4, 4, -4, -1))
 coef.est.2[seg + 1, 2:6] <- t(summary(ml_est_sub)$coefficient)
 se.est.2[seg + 1, 2:6] <- t(sqrt(abs(diag(solve(ml_est_sub$hessian)))))

}
coef.est.2
plot(coef.est.2$intercept.1 - coef.est.2$intercept.3, coef.est.2$last.purchase)
plot(coef.est.2$price, coef.est.2$last.purchase)
#####################################
# targeting
#####################################
# if we can target each segment, will charge them VERY different prices
profit.sim.type <- function(p, coef) {
 share <- calc_shares(coef, p)
 profit <- share * (p - mc)
 return(profit)
}
# solve for profit-maximization problem by segment
optim.kiwi.profit.1 <- optim(c(1.43, 1.43), function(prices)
 -sum(profit.sim.type(t(c(prices, 1.43)), c(beta.1, alpha.1))[1:2]))
p.1 <- optim.kiwi.profit.1$par
optim.kiwi.profit.2 <- optim(c(1.43, 1.43), function(prices)
 -sum(profit.sim.type(t(c(prices, 1.43)), c(beta.2, alpha.2))[1:2]))
p.2 <- optim.kiwi.profit.2$par
# for segment 1 we price at:
p.1
# for segment 2 we price at:
p.2
###### now: systematically do targeting ######
# first, in-sample prediction
likelihood.individual.1 <- likelihood(c(beta.1, alpha.1), p, choice, id)
likelihood.individual.2 <- likelihood(c(beta.2, alpha.2), p, choice, id)
# posterior probability of being segment 1 (1 obs per consumer)
posterior.prob.1 <- (likelihood.individual.1 * type.prob) /
 (likelihood.individual.1 * type.prob + likelihood.individual.2 * (1 - type.prob))
posterior.prob.1[, 1] <- as.integer(row.names(posterior.prob.1))
colnames(posterior.prob.1) <- c("id", "posterior.type.prob")
# distribution of this posterior probability?
hist(posterior.prob.1[, 2], xlab = 'posterior prob of segment 1', main = '')
# uninformed price ($1.15, $1.03)
p.0 <- optim.kiwi.profit$par
# merge posterior probability back to data
data.target.in <- merge(data, posterior.prob.1, by = "id")
# categorize as type 1 if type 1 probability greater than 90%
data.target.in$target.category <- (data.target.in$posterior.type.prob >= 0.90) * 1 +
 (data.target.in$posterior.type.prob <= 0.10) * 2
# assign targeted price
data.target.in$targeted.price.kr <- (data.target.in$target.category == 1) * p.1[1] +
 (data.target.in$target.category == 2) * p.2[1] +
 (data.target.in$target.category == 0) * p.0[1]
data.target.in$targeted.price.kb <- (data.target.in$target.category == 1) * p.1[2] +
 (data.target.in$target.category == 2) * p.2[2] +
 (data.target.in$target.category == 0) * p.0[2] 
 in.samp.price <- cbind(data.target.in$targeted.price.kr,
data.target.in$targeted.price.kb, 1.43)
profit.sim.posterior <- function(p, trip = data$trip, s = 0) {
 share.1 <- calc_shares(c(beta.1, alpha.1), p)
 share.2 <- calc_shares(c(beta.2, alpha.2), p)
 share <- data.target.in$posterior.type.prob * share.1 +
 (1 - data.target.in$posterior.type.prob) * share.2
 profit <- share * (p - mc)
 profit <- profit[trip > s, 1] + profit[trip > s, 2]
 profit <- aggregate(x = profit, by = list(subset(data, trip > s)$id), FUN = mean)
 return(profit)
}
profit.in.samp <- profit.sim.posterior(in.samp.price)
hist(profit.in.samp[, 2], xlab = 'average profit per trip among customers', main = '')
# compare: original price, no segmentation, segmentation, targeted, theoretical
original.profit <- sum(profit.sim(t(c(1.37, 1.37, 1.43)))[1:2])
theoretical.profit <- - type.prob * optim.kiwi.profit.1$value - (1 - type.prob) *
optim.kiwi.profit.2$value
naive.profit <- sum(profit.sim(t(c(optim.kiwi.simp$par, 1.43)))[1:2])
profit_vec <- c(original.profit, naive.profit, -optim.kiwi.profit$value,
mean(profit.in.samp[, 2]), theoretical.profit) * 1000
names(profit_vec) <- c('data', 'no seg/pos', 'seg/pos', 'targeted', 'theoretical')
barplot(profit_vec, col = 'white', ylim = c(250, 420), ylab = 'profit', xpd = FALSE)
